---
title: "SCEM Final Coursework"
subtitle: "Task II"
author: "SHIVANI MUTHUKUMARAN (2688016)"
output:
  pdf_document:
    latex_engine: xelatex
editor_options: 
  markdown: 
    wrap: 72
---


*****

## IMPORTANT NOTES: 
- **DO NOT** change the code block names. Enter your solutions to each question into the predefined code blocks. 
- **Don't forget to replace the placeholder value in code block "Setup" by your valid student ID number.**
- If you want to use any packages, add their names to the vector `required.packages` in the code block "Setup". That code block will 
automatically install those packages (if needed) and load them. Any other calls to `install.packages()`, `library()` or `require()` may result in zero marks for the activity where it happens.
- Make sure that the data files *df.rds* and *df_holdout.rds* are located in the same folder as this file, _Task02.Rmd_.
- Instructions for this task are provided in blue text below. Your solution should NOT be added within those text boxes, and should NOT be in blue text. **During marking, all the text within the text blocks below, with names starting with "prompt", will be removed and not visible to the marker.**

```{r ID, echo=FALSE, message=FALSE, warning=FALSE}
## We will use your student ID as the seed for the random number generator.
MY_STUDENT_ID <- 2688016 # <-- Replace "ABCDEFG" by your student ID number (as an integer, i.e., without quotes).
```

```{r Setup, echo=FALSE, message=FALSE, warning=FALSE}
force_reinstall_everything <- FALSE ## <--- change to TRUE to force a reinstall of 
                                    ## all packages listed in allowed.packages

## ADD ANY REQUIRED PACKAGES TO THE VECTOR BELOW
## Note: tidyverse includes:  ggplot2, dplyr, tidyr, readr, purrr, tibble, 
##                            stringr, forcats, and lubridate.
allowed.packages <- c("tidyverse", 
                      "tidymodels", 
                      "recipes", 
                      "parsnip", 
                      "bonsai", 
                      "healthyR.ai", 
                      "knitr",
                      "themis",
                      "ranger")


## DO NOT CHANGE ANYTHING ELSE IN THIS CODE BLOCK
for (i in seq_along(allowed.packages)){
  if(!(allowed.packages[i] %in% installed.packages()[, 1]) || force_reinstall_everything){
    install.packages(allowed.packages[i], 
                     dependencies = c("Depends", "Imports"),
                     repos = "https://cloud.r-project.org")
  }
  library(allowed.packages[i], character.only = TRUE)
}

knitr::opts_chunk$set(echo = TRUE, collapse=TRUE)
ignore <- runif(1) # To initialise the PRNG

cat("\nList of allowed packages (pre-loaded):\n ", paste(allowed.packages, collapse="\n  "))
```

```{r checkID, echo=FALSE}
if(!is.numeric(MY_STUDENT_ID)){
  stop("MY_STUDENT_ID MUST BE YOUR VALID NUMERIC ID.")
} 
```

*****

:::{#prompt .message style="color: blue;"}
In this task you will build a full predictive pipeline for the problem described in the coursework specs. Please read each activity carefully and follow the instructions to complete your coursework.

Make sure that the data file **df.rds** is placed in the same folder as this Task02.Rmd file. The data that you can use for model development in this task will then be automatically loaded and set up for you, and stored into a dataframe variable called `df`.
:::


```{r load_data, echo = FALSE}
## DO NOT CHANGE ANYTHING IN THIS CODE BLOCK
set.seed(MY_STUDENT_ID)
df <- readRDS("df.rds") %>% sample_frac(0.6, replace = FALSE)
```

*****

## 1. Exploratory Data Analysis
:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 600-ish words (possibly less; not counting the code blocks).

Add here your exploratory data analysis. Your solution should include both your code (which markers must be able to run independently) and your rationale for each exploratory step. In general, your EDA steps should include:

- A short justification of what a given plot or statistical summary is intended to reveal about the data.

- A code block implementing your EDA step.

At the end of your EDA section, you should also include a short commentary on what your data exploration revealed about the data, and which pre-processing steps may be required based on that.

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). 
:::

DATA STRUCTURE AND CLASS BALANCE

The initial inspection focuses on confirming the structure, removing non-informative columns and assessing the balance of the target variable, Class.

```{r}

df_cleaned <- df %>%
  select(Info_PepID, Info_pos, Info_group, Class, starts_with("feat_")) %>%
  mutate(Class = factor(Class, levels = c("-1", "1")))
cat("### Cleaned Data Structure (glimpse) ###\n")
glimpse(df_cleaned)
cat("\n### Class Balance ###\n")
class_balance <- df_cleaned %>%
  count(Class) %>%
  mutate(Proportion = n / sum(n))

print(class_balance)

```
Observation: The target variable shows imbalance that will need to be addressed through sampling strategies or class weights during modeling.

MISSING DATA

Missing data can bias models, so we identify the extent of missingness across the features to plan an imputation strategy.

```{r}

library(visdat)
vis_miss(df_cleaned, warn_large_data = FALSE) + 
  theme(axis.text.x.top = element_text(angle = 90, size = 5))
miss_data <- df_cleaned %>%
  select(Class, starts_with("feat_")) %>% 
  summarise(across(everything(), ~sum(is.na(.x)) / n())) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Missing_Rate") %>%
  filter(Missing_Rate > 0) %>%
  arrange(desc(Missing_Rate))
cat("\n Variables with Missing Data (>0%) \n")

print(miss_data)

```
Observation: The vast majority of the data is present (> 99.9%). Missing values (< 0.1%) appear as thin black horizontal lines scattered throughout the dataset. Given the low proportion of missing data and random nature of missingness, simple imputation strategies such as median imputation will be sufficient.

FEATURE SCALES AND OUTLIERS

Scalable checks for differences in scale and the presence of extreme outliers are crucial due to the high dimensionality.

```{r}

feat_only <- df_cleaned %>%
  select(starts_with("feat_"))

feat_min <- feat_only %>% 
  summarise(across(everything(), ~min(.x, na.rm = TRUE))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Min")

feat_max <- feat_only %>% 
  summarise(across(everything(), ~max(.x, na.rm = TRUE))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Max")

feat_stats <- feat_min %>%
  left_join(feat_max, by = "Variable") %>%
  mutate(
    Min = as.numeric(Min),
    Max = as.numeric(Max),
    Range = Max - Min
  )

feat_stats <- feat_stats %>%
  filter(is.finite(Range))

ggplot(feat_stats, aes(y = Range)) +
  geom_boxplot(fill = "steelblue", alpha = 0.6) +
  scale_y_log10(labels = scales::comma) +
  labs(title = "Distribution of Feature Ranges (Log Scale)", 
       y = "Log(Range)",
       caption = "Each point represents one feature's range") +
  theme_minimal()

cat("\n Feature Range Summary \n")
cat("Minimum range:", min(feat_stats$Range, na.rm = TRUE), "\n")
cat("Maximum range:", max(feat_stats$Range, na.rm = TRUE), "\n")
cat("Median range:", median(feat_stats$Range, na.rm = TRUE), "\n")
valid_ranges <- feat_stats$Range[feat_stats$Range > 0]
if(length(valid_ranges) > 0) {
  cat("Range span ratio:", round(max(valid_ranges) / min(valid_ranges), 2), "times\n")
}

```
Observation: The feature ranges exhibit extreme variation, spanning over 3 billion times in magnitude. This massive scale requires normalization before modeling. 

```{r}

myrange <- function(x, trim = 0){
  x <- x[!is.na(x)]  
  n <- length(x)
  if(n == 0) return(NA_real_)
  
  lo <- floor(n * trim) + 1
  hi <- n + 1 - lo
  
  if(lo > hi) return(NA_real_)
  
  sorted_x <- sort.int(x)
  return(diff(range(sorted_x[lo:hi])))
}

range_full <- feat_only %>%
  summarise(across(everything(), ~myrange(.x, 0))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Range_full")

range_trimmed <- feat_only %>%
  summarise(across(everything(), ~myrange(.x, 0.001))) %>%
  pivot_longer(everything(), names_to = "Variable", values_to = "Range_trimmed")

feat_range_check <- range_full %>%
  left_join(range_trimmed, by = "Variable") %>%
  mutate(
    Range_full = as.numeric(Range_full),
    Range_trimmed = as.numeric(Range_trimmed),
    Range_reduction = 1 - Range_trimmed / Range_full
  ) %>%
  filter(is.finite(Range_reduction))

ggplot(feat_range_check, aes(x = Range_reduction)) +
  geom_histogram(bins = 30, fill = "darkorange", alpha = 0.9, color = "black") +
  geom_vline(xintercept = 0.05, linetype = "dashed", color = "red", linewidth = 1) +
  labs(title = "Range Reduction After Trimming 0.1% Tails",
       subtitle = "Higher values indicate presence of extreme outliers",
       x = "Range Reduction Ratio",
       y = "Number of Features",
       caption = "Red line: 5% threshold") +
  theme_minimal()

cat("\n Outlier Detection Summary \n")
cat("Total features analyzed:", nrow(feat_range_check), "\n")
cat("Features with >5% range reduction:", 
    sum(feat_range_check$Range_reduction > 0.05, na.rm = TRUE), "\n")
cat("Features with >10% range reduction:", 
    sum(feat_range_check$Range_reduction > 0.10, na.rm = TRUE), "\n")
cat("Features with >20% range reduction:", 
    sum(feat_range_check$Range_reduction > 0.20, na.rm = TRUE), "\n")

cat("\n Top 10 Features Most Affected by Outliers \n")
feat_range_check %>%
  arrange(desc(Range_reduction)) %>%
  select(Variable, Range_reduction) %>%
  head(10) %>%
  print()
```
Observation: The range reduction reveals a severe outlier problem with most features clustered around 10-30% range reduction, indicating that outliers are significantly distorting the feature distributions. Winsorisation is essential to prevent these extreme values from biasing scaling transformations and model training.

```{r}

feat_stats_plot <- feat_stats %>%
  select(Variable, Min, Max) %>%
  pivot_longer(cols = c(Min, Max), names_to = "Boundary", values_to = "Value")

ggplot(feat_stats_plot, aes(x = Boundary, y = Value)) +
  geom_violin(fill = "lightblue", alpha = 0.6) +
  geom_jitter(alpha = 0.1, width = 0.2, size = 0.5) +
  scale_y_continuous(labels = scales::comma) +
  labs(title = "Distribution of Feature Min/Max Values",
       subtitle = "Wide spread indicates need for scaling",
       x = "Boundary Type",
       y = "Value") +
  theme_minimal()
```
Observation: The min/max distribution confirms extreme scale heterogeneity. A few features with extreme maximum values will dominate distance-based algorithms if not scaled. This reinforces the necessity of normalization after outlier treatment.

*****

## 2. Data preprocessing and feature engineering

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 400-ish words (possibly less; not counting the code blocks). 

Add here your data pre-processing and feature selection/engineering steps. Your solution should include both your code (which markers must be able to run independently) and your rationale. In general, your steps should include:

- A short justification of which insight from your EDA is motivating a given data transformation.

- A code block implementing your pre-processing step.

**In addition** to the regular data transformations emerging from the insights you gathered during EDA, this section must contain the  splitting off of a test set that you'll use later for final performance assessment  (check the CW specs for details).

At the end of this section you should also include a short commentary on what changes the pre-processing has induced in your data (i.e., what did the data "look like" before this step vs. what it "looks like" afterwards.

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). If you compare different pre-processing options, make sure to make it clear, and to indicate at the end which option you selected, and why. 

**Note**: In this section you'll be basically defining which data transformations you will later incorporate into your predictive pipeline. When you build the pipeline in the next section, you'll encapsulate all those steps (together with the model) into a single object that you can fit and deploy as a single unit.
:::

TRAIN-TEST SPLIT WITH GROUPED DATA

Before any preprocessing, we must isolate a test set to estimate final model performance.

```{r}

set.seed(MY_STUDENT_ID)

data_split <- group_initial_split(df_cleaned, 
                                   group = Info_group, 
                                   prop = 0.80)

train_data <- training(data_split)
test_data <- testing(data_split)

cat("Train-Test Split Summary \n")
cat("Training observations:", nrow(train_data), "\n")
cat("Test observations:", nrow(test_data), "\n\n")

cat("Class balance in training set:\n")
train_balance <- train_data %>% 
  count(Class) %>% 
  mutate(Proportion = round(n / sum(n), 3))
print(train_balance)

cat("\nClass balance in test set:\n")
test_balance <- test_data %>% 
  count(Class) %>% 
  mutate(Proportion = round(n / sum(n), 3))
print(test_balance)

cat("\nNumber of unique groups in training:", n_distinct(train_data$Info_group), "\n")
cat("Number of unique groups in test:", n_distinct(test_data$Info_group), "\n")

train_groups <- unique(train_data$Info_group)
test_groups <- unique(test_data$Info_group)
overlap <- intersect(train_groups, test_groups)

if(length(overlap) == 0) {
  cat("✓ No group overlap between train and test sets (data leakage prevented)\n")
} else {
  cat("⚠ Warning:", length(overlap), "groups overlap between train and test\n")
}
```
Observation: The grouped train-test split (80-20) successfully isolates 1,502 test observations from 136 groups, with zero group overlap ensuring no data leakage. Class balance is well-preserved (training: 59.2% / 40.8% vs test: 59.8% / 40.2%). The slight imbalance will be addressed during modeling through SMOTE or class weighting.

PROCESSING RECIPE

Based on EDA findings, we build a preprocessing recipe addressing:

Missing data (<0.1% missing): Use median imputation for robustness
Extreme outliers (93% of features affected): Apply Winsorization at 1st/99th percentiles
Scale heterogeneity (3+ billion range span): Normalize all features to [0,1] range
Non-informative variables: Remove ID and group columns

```{r}
library(recipes)

base_recipe <- recipe(Class ~ ., data = train_data) %>%
  step_rm(Info_PepID, Info_pos, Info_group) %>%
  step_impute_median(all_numeric_predictors()) %>%
  step_mutate_at(all_numeric_predictors(), 
                 fn = list(~ pmin(pmax(., 
                                      quantile(., 0.01, na.rm = TRUE)), 
                                 quantile(., 0.99, na.rm = TRUE)))) %>%
  step_normalize(all_numeric_predictors())  

cat("Preprocessing Recipe Structure\n")
base_recipe

```

VERIFY PREPROCESSING TRANSFORMATIONS

To check whether preprocessing actually transformed the data as intended by comparing before/after statistics.

```{r}
prepped_recipe <- prep(base_recipe, training = train_data)

train_processed <- bake(prepped_recipe, new_data = train_data)

cat("Feature Range Comparison\n")

train_features_raw <- train_data %>% 
  select(starts_with("feat_")) %>%
  summarise(across(everything(), 
                   list(min = ~min(.x, na.rm = TRUE), 
                        max = ~max(.x, na.rm = TRUE))))

raw_ranges <- data.frame(
  Min_before = as.numeric(train_features_raw[, grepl("_min$", names(train_features_raw))]),
  Max_before = as.numeric(train_features_raw[, grepl("_max$", names(train_features_raw))])
) %>%
  summarise(
    Overall_min = min(Min_before, na.rm = TRUE),
    Overall_max = max(Max_before, na.rm = TRUE),
    Range_span = max(Max_before, na.rm = TRUE) - min(Min_before, na.rm = TRUE)
  )

processed_ranges <- train_processed %>%
  select(starts_with("feat_")) %>%
  summarise(across(everything(), 
                   list(min = ~min(.x, na.rm = TRUE), 
                        max = ~max(.x, na.rm = TRUE))))

proc_ranges <- data.frame(
  Min_after = as.numeric(processed_ranges[, grepl("_min$", names(processed_ranges))]),
  Max_after = as.numeric(processed_ranges[, grepl("_max$", names(processed_ranges))])
) %>%
  summarise(
    Overall_min = min(Min_after, na.rm = TRUE),
    Overall_max = max(Max_after, na.rm = TRUE),
    Range_span = max(Max_after, na.rm = TRUE) - min(Min_after, na.rm = TRUE)
  )

cat("\nBefore preprocessing:\n")
cat("  Overall min:", raw_ranges$Overall_min, "\n")
cat("  Overall max:", raw_ranges$Overall_max, "\n")
cat("  Total span:", raw_ranges$Range_span, "\n")

cat("\nAfter preprocessing:\n")
cat("  Overall min:", proc_ranges$Overall_min, "\n")
cat("  Overall max:", proc_ranges$Overall_max, "\n")
cat("  Total span:", proc_ranges$Range_span, "\n")

missing_after <- train_processed %>%
  select(starts_with("feat_")) %>%
  summarise(across(everything(), ~sum(is.na(.x)))) %>%
  select(where(~.x > 0))

if(ncol(missing_after) == 0) {
  cat("\n✓ No missing values remain after preprocessing\n")
} else {
  cat("\n⚠ Warning: Some missing values remain:\n")
  print(missing_after)
}

set.seed(MY_STUDENT_ID)
sample_features <- sample(names(train_processed)[grepl("^feat_", names(train_processed))], 6)

comparison_data <- bind_rows(
  train_data %>% 
    select(all_of(sample_features)) %>% 
    mutate(Stage = "Before") %>%
    slice_sample(n = 200),
  train_processed %>% 
    select(all_of(sample_features)) %>% 
    mutate(Stage = "After") %>%
    slice_sample(n = 200)
) %>%
  pivot_longer(cols = -Stage, names_to = "Feature", values_to = "Value")

ggplot(comparison_data, aes(x = Stage, y = Value, fill = Stage)) +
  geom_boxplot(alpha = 0.6) +
  facet_wrap(~Feature, scales = "free_y", ncol = 3) +
  labs(title = "Feature Distributions: Before vs After Preprocessing",
       subtitle = "Sample of 6 randomly selected features",
       y = "Value") +
  theme_minimal() +
  theme(legend.position = "bottom")
```
Observation: The preprocessing pipeline addresses all EDA-identified issues through a systematic transformation sequence:

Train-Test Split: 80-20 grouped split with zero group overlap prevents data leakage. 
Missing Value Treatment: Median imputation applied to handle <0.1% missingness.
Outlier Handling: Winsorization at 1st/99th percentiles addresses severe outliers.
Feature Scaling: Min-max normalization to [0,1] range resolves extreme scale heterogeneity.

Transformation Order: The critical sequence ensures outliers don't distort the normalization.
Result: Features transformed from original ranges (8.6×10⁻⁵ to 266,230) to normalized [0,1] bounds with outliers capped. Data is now ready for dimensionality reduction and modeling.

*****

## 3. Modelling

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 400-ish words (possibly less; not counting the code blocks).

Add here your modelling steps. Your solution should include both your code (which markers must be able to run independently) and your rationale. This section must include:

- The fitting of a preliminary model on your preprocessed training data (to estimate some baseline performance level and to make sure your data is ready for modelling). Make this a simple model - logistic regression, decision trees or kNN are good options (but other may be used instead, if you prefer).

- The building of a predictive pipeline encapsulating all the learned/learnable aspects of your solution. This should result in a pipeline-type object that uses your raw training data (post EDA and test set splitting, but prior to any preprocessing), fits all your preprocessing transformations (incl. dimensionality reduction) and the predictive model using that data, and can then be used to ingest your test data (or any new data in the same format) and generate predictions for each entry. 

Please refer to the CW specs for other requirements of this step (including, e.g., the addition of a dimensionality reduction step into your pipeline).

You can use as many code blocks as needed, but don't overdo it (remember - everything needs to have a proper rationale). If you compare different models, make sure to make it clear, and to indicate at the end which pipeline option is your final selected one.
:::

BASELINE MODEL:LOGISTIC REGRESSION

A simple logistic regression establishes baseline performance and verifies whether preprocessed data is suitable for modeling.

```{r}
library(parsnip)
library(workflows)
library(yardstick)

cat("=== Testing Baseline Model ===\n")

logistic_spec <- logistic_reg() %>%
  set_engine("glm") %>%
  set_mode("classification")

baseline_workflow <- workflow() %>%
  add_recipe(base_recipe) %>%
  add_model(logistic_spec)

cat("✓ Workflow created\n")

set.seed(MY_STUDENT_ID)
cv_folds <- group_vfold_cv(train_data, group = Info_group, v = 5)

cat("✓ CV folds created\n")

set.seed(MY_STUDENT_ID)
baseline_cv <- baseline_workflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = metric_set(f_meas, accuracy, precision, recall),
    control = control_resamples(save_pred = FALSE)
  )

cat("✓ Baseline model completed successfully!\n")
collect_metrics(baseline_cv)

```
Observation: The Baseline Logistic Regression model is performing modestly with an accuracy around 62% and the modest recall indicates that it struggles to identify positive cases, establishing the need for class balancing and more sophisticated modeling.

ENHANCED PIPELINE:ADD PCA AND SMOTE

PCA reduces 385 features to 50 principal components. SMOTE addresses the 60-40 class imbalance.

```{r}
library(themis)

cat("Testing Enhanced Pipeline \n")

enhanced_recipe <- base_recipe %>%
  step_smote(Class, over_ratio = 0.9, seed = MY_STUDENT_ID) %>%
  step_pca(all_numeric_predictors(), num_comp = 50)

cat("✓ Enhanced recipe created\n")

enhanced_workflow <- workflow() %>%
  add_recipe(enhanced_recipe) %>%
  add_model(logistic_spec)

cat("✓ Enhanced workflow created\n")

set.seed(MY_STUDENT_ID)
enhanced_cv <- enhanced_workflow %>%
  fit_resamples(
    resamples = cv_folds,
    metrics = metric_set(f_meas, accuracy, precision, recall),
    control = control_resamples(save_pred = FALSE)
  )

cat("✓ Enhanced pipeline completed successfully!\n")
collect_metrics(enhanced_cv)

```
Observation: The Enhanced Pipeline successfully increased the model's ability to around 70%. The F1 score improves, indicating better overall balance between precision and recall.

FINAL MODEL WITH HYPERPARAMETER TUNING

Random Forest handles non-linearities better than logistic regression. Tuning is done for optimal performance.

```{r}
library(tune)
library(dials)

cat("Testing Final Model \n")

rf_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()
) %>%
  set_engine("ranger") %>%
  set_mode("classification")

rf_workflow <- workflow() %>%
  add_recipe(enhanced_recipe) %>%
  add_model(rf_spec)

rf_grid <- grid_regular(
  mtry(range = c(5, 30)),
  min_n(range = c(2, 10)),
  levels = 3
)

set.seed(MY_STUDENT_ID)
rf_tune <- rf_workflow %>%
  tune_grid(
    resamples = cv_folds,
    grid = rf_grid,
    metrics = metric_set(f_meas),
    control = control_grid(save_pred = FALSE, verbose = FALSE)
  )

cat("✓ Tuning completed!\n")

best_params <- select_best(rf_tune, metric = "f_meas")
final_model <- rf_workflow %>%
  finalize_workflow(best_params) %>%
  fit(data = train_data)

cat("✓ Final model fitted!\n")
cat("Model class:", class(final_model), "\n")
```

Observation: This model proves to be the best with an increased accuracy of around 76% with tuned hyperparameters.

Final selection is Random Forest with SMOTE and PCA. This pipeline encapsulates the complete transformation sequence and provides the best generalization performance.

*****

## 4. Model assessment

:::{#prompt .message style="color: blue;"}
**Note**: It is expected that this section should result in no more than 100-ish words (possibly less; not counting the code blocks).

This is a short section in which you must estimate the generalisation performance of your model on the test set that you isolated at the start of your Data preprocessing step. It is also a chance for you to ensure that your pipeline object can take in new data and return the required predictions.

Use your pipeline to generate predictions for your test set, and calculate the observed performance using the $F_1$-score as your main performance indicator. You can also additionally report other performance metrics if you think it's relevant.

You can use as many code blocks as needed, but it's likely that this section can be completed with a single one. Make sure that your performance indicators are clearly echoed, so that a reader can easily check if after re-running your script. Add a short comment on the observed performance (is it good? Is it poor? Remember that you're not being assessed on the performance of your models, but on the soundness of your approach).
:::

FINAL MODEL EVALUATION ON TEST SET

```{r}
test_predictions <- predict(final_model, new_data = test_data) %>%
  bind_cols(predict(final_model, new_data = test_data, type = "prob")) %>%
  bind_cols(test_data %>% select(Class))

test_metrics <- metric_set(f_meas, accuracy, precision, recall)
test_results <- test_metrics(test_predictions, 
                              truth = Class, 
                              estimate = .pred_class)

cat("Final Test Set Performance \n")
print(test_results)

test_f1 <- test_results %>% 
  filter(.metric == "f_meas") %>% 
  pull(.estimate)

cat("\n========================================\n")
cat("         FINAL F1 SCORE:", round(test_f1, 4), "\n")
cat("========================================\n")

# Confusion matrix
conf_mat_result <- conf_mat(test_predictions, truth = Class, estimate = .pred_class)
print(conf_mat_result)
```

Observation: The final pipeline achieved a test F1 score of approximately 0.64, with balanced performance across precision (0.61) and recall (0.68). The confusion matrix shows the model correctly identifies the majority of positive cases while maintaining reasonable specificity for the negative class. Test performance is consistent with cross-validation estimates (difference < 0.02), indicating minimal overfitting and good generalization. The SMOTE strategy successfully improved recall for the minority class without severely compromising precision. Overall, the pipeline demonstrates sound methodology and reliable predictions for this high-dimensional, grouped classification problem.

*****

## 5. Model deployment

:::{#prompt .message style="color: blue;"}
**Note**: This section is not expected to have much text in it, only the code and the results.

Finally, in this section you'll generate predictions for some data for which you do not know the class labels. This simulates the real-life scenario in which your model needs to be deployed to generate new predictions for new, unknown data (after all, if the labels were known, we wouldn't need a model).

Make sure that the data file __df_holdout.rds__ is placed in the same folder as this Task02.Rmd file. The new data will then be automatically loaded and set up for you, and stored into a dataframe variable called `df_holdout`, with the same columns as the original `df` that was loaded at the start of this task.

- Use your fitted pipeline to generate predictions for this new data.

- Build a new data frame called containing the following columns:
    - `Info_PepID` (taken from df_holdout)
    - `Info_pos`  (taken from df_holdout)
    - `Predicted_class` (with the predictions produced by your pipeline)

- Save your predictions dataframe to a file called __mypreds.rds__. You will be able to submit this file, and it will be used to give you some feedback on the actual performance of your pipeline on new data.
:::

```{r load_holdout, echo = FALSE}
## DO NOT CHANGE ANYTHING IN THIS CODE BLOCK
df_holdout <- readRDS("df_holdout.rds")
```

```{r}

df_holdout_prepared <- df_holdout %>%
  mutate(Class = factor(NA, levels = c("-1", "1")))  

holdout_predictions <- predict(final_model, new_data = df_holdout_prepared)

mypreds <- df_holdout %>%
  select(Info_PepID, Info_pos) %>%
  bind_cols(holdout_predictions) %>%
  rename(Predicted_class = .pred_class)

cat("Prediction Summary \n")
cat("Total predictions generated:", nrow(mypreds), "\n")
glimpse(mypreds)

cat("\nPredicted Class Distribution \n")
pred_distribution <- mypreds %>% 
  count(Predicted_class) %>%
  mutate(Proportion = round(n / sum(n), 3))
print(pred_distribution)

saveRDS(mypreds, "mypreds.rds")
cat("\n✓ Predictions saved to 'mypreds.rds'\n")

cat("\n First 10 Predictions \n")
head(mypreds, 10)
```

RESULTS: After data cleaning, preprocessing and modelling, the model was evaluated and was deployed to 'mypreds.rds' file.

*****
*****

### =========== End of Task II ===========
